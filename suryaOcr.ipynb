{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "from surya.detection import batch_text_detection\n",
    "from surya.layout import batch_layout_detection\n",
    "from surya.model.detection.segformer import load_model, load_processor\n",
    "from surya.settings import settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Scan:\n",
    "    def __init__(self, oriImg) :\n",
    "        self.oriImg = oriImg\n",
    "\n",
    "\n",
    "    def adjust(self, img):\n",
    "        # # 이미지 전처리 및 외곽선 추출\n",
    "        edged = self.extractEdge(img)\n",
    "        \n",
    "        contours, hierarchy = cv2.findContours(edged.copy(), cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)\n",
    "        draw = img.copy()\n",
    "        cv2.drawContours(draw,  contours, -1, (0, 255, 0))\n",
    "        \n",
    "        # 사각형 중 최대크기의 컨투어 꼭지점\n",
    "        pts = self.getPointsOfMaxRectangle(contours)\n",
    "\n",
    "        # 각각의 좌표 찾기\n",
    "        sumXY = pts.sum(axis=1)\n",
    "        diff = np.diff(pts, axis=1)\n",
    "\n",
    "        topLeft = pts[np.argmin(sumXY)]\n",
    "        bottomRight = pts[np.argmax(sumXY)]\n",
    "        topRight = pts[np.argmin(diff)]\n",
    "        bottomLeft = pts[np.argmax(diff)]\n",
    "\n",
    "        # 사진을 변환할 때 사용할 서류의 높이\n",
    "        widthTop = abs(topRight[0] - topLeft[0])\n",
    "        widthBottom = abs(bottomRight[0] - bottomLeft[0])\n",
    "        heightRight = abs(topRight[1] - bottomRight[1])\n",
    "        heightLeft = abs(topLeft[1] - bottomLeft[1])\n",
    "        print(widthBottom, widthTop, heightLeft, heightRight)\n",
    "\n",
    "        width = max([widthTop, widthBottom])\n",
    "        height = max([heightRight, heightLeft])\n",
    "\n",
    "        pts1 = np.float32([topLeft, topRight, bottomRight, bottomLeft])\n",
    "        pts2 = np.float32([[0,0], [width, 0], [width, height], [0, height]])\n",
    "\n",
    "        matrix = cv2.getPerspectiveTransform(pts1, pts2) # 좌표를 변환하기 위해 사용할 변환행렬\n",
    "        result = cv2.warpPerspective(img, matrix, (width, height)) # 이미지 변환(변환행렬 적용)\n",
    "\n",
    "        return result\n",
    "    \n",
    "    def extractEdge(self, img):\n",
    "        # 이미지 전처리 및 외곽선 추출\n",
    "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "        gray = cv2.GaussianBlur(gray, (3, 3), 0) # 이미지를 흐리게 처리함 (noise 제거를 위해 사용)\n",
    "        edged = cv2.Canny(gray, 75, 250) # edged를 검출하는 함수 (img, minVal, maxVal)\n",
    "        return edged\n",
    "    \n",
    "    def getPointsOfMaxRectangle(self, contours) :\n",
    "        # 크기순으로 컨투어 정렬\n",
    "        contours = sorted(contours, key=cv2.contourArea, reverse=True)[:5]\n",
    "        for c in contours:\n",
    "            peri = cv2.arcLength(c, True) # 외곽선 길이\n",
    "            # print(peri)\n",
    "            verticles = cv2.approxPolyDP(c, 0.02 * peri, closed=True) # 외곽선 근사화\n",
    "            if len(verticles) == 4 : \n",
    "                break\n",
    "        pts = verticles.reshape(4, 2) # 배열을 4 * 2 크기로 조정\n",
    "        return pts\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2317 1969 2939 3018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\anjp\\.conda\\envs\\ocr_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded detection model vikp/surya_layout2 on device cpu with dtype torch.float32\n",
      "Loaded detection model vikp/surya_det2 on device cpu with dtype torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Detecting bboxes: 100%|██████████| 1/1 [00:25<00:00, 25.35s/it]\n",
      "Detecting bboxes: 100%|██████████| 1/1 [00:16<00:00, 16.73s/it]\n"
     ]
    }
   ],
   "source": [
    "# image = Image.open(\"./dataset/KakaoTalk_20240420_113433452.jpg\")\n",
    "img = cv2.imread(\"./dataset/KakaoTalk_20240420_113433452_01.jpg\")\n",
    "scanner = Scan(img)\n",
    "\n",
    "image= scanner.adjust(img)\n",
    "cv2.imwrite(\"./result.jpg\", image)\n",
    "\n",
    "image = Image.open(\"./result.jpg\")\n",
    "model = load_model(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)\n",
    "processor = load_processor(checkpoint=settings.LAYOUT_MODEL_CHECKPOINT)\n",
    "det_model = load_model()\n",
    "det_processor = load_processor()\n",
    "\n",
    "# layout_predictions is a list of dicts, one per image\n",
    "\n",
    "line_predictions = batch_text_detection([image], det_model, det_processor)\n",
    "layout_predictions = batch_layout_detection([image], model, processor, line_predictions)\n",
    "\n",
    "# langs = [\"en\"]\n",
    "# det_processor, det_model = segformer.load_processor(), segformer.load_model()\n",
    "# rec_model, rec_processor = load_model(), load_processor()\n",
    "\n",
    "# predictions = run_ocr([image], [langs], det_model, det_processor, rec_model, rec_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[877, 77, 1454, 104]\n",
      "[1174, 1629, 1482, 1664]\n",
      "[1169, 169, 2181, 1548]\n",
      "[138, 165, 1158, 2647]\n",
      "[1160, 1694, 2194, 2656]\n",
      "[144, 2702, 561, 2728]\n"
     ]
    }
   ],
   "source": [
    "bboxArr = []\n",
    "for b in layout_predictions[0].bboxes:\n",
    "    print(b.bbox)\n",
    "    bboxArr.append(b.bbox)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded reading order model vikp/surya_order on device cpu with dtype torch.float32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Finding reading order: 100%|██████████| 1/1 [00:05<00:00,  5.12s/it]\n"
     ]
    }
   ],
   "source": [
    "from surya.ordering import batch_ordering\n",
    "from surya.model.ordering.processor import load_processor as order_load_processor\n",
    "from surya.model.ordering.model import load_model as order_load_model\n",
    "\n",
    "ord_model = order_load_model()\n",
    "ord_processor = order_load_processor()\n",
    "\n",
    "order_predictions = batch_ordering([image], [bboxArr], ord_model, ord_processor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(877, 77) (1454, 104)\n",
      "0\n",
      "(138, 165) (1158, 2647)\n",
      "1\n",
      "(144, 2702) (561, 2728)\n",
      "2\n",
      "(1169, 169) (2181, 1548)\n",
      "3\n",
      "(1174, 1629) (1482, 1664)\n",
      "4\n",
      "(1160, 1694) (2194, 2656)\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# 분석한 레이아웃에 따라 읽기\n",
    "order_predictions[0].bboxes.sort(key=lambda e : e.position)\n",
    "order_predictions[0].bboxes\n",
    "\n",
    "layout_img = cv2.imread(\"./result.jpg\")\n",
    "for b in order_predictions[0].bboxes:\n",
    "    start = (int(b.bbox[0]), int(b.bbox[1]))\n",
    "    end = (int(b.bbox[2]), int(b.bbox[3]))\n",
    "    print(start, end)\n",
    "    cv2.rectangle(layout_img, start, end, (255, 255, 255), 2)\n",
    "    #텍스트 추가하기\n",
    "    print(b.position)\n",
    "    cv2.putText(layout_img, str(b.position), start, cv2.FONT_HERSHEY_SIMPLEX, 2, 2, 2)\n",
    "\n",
    "cv2.namedWindow(\"test\", cv2.WINDOW_NORMAL)\n",
    "\n",
    "cv2.imshow(\"test\", layout_img)\n",
    "cv2.waitKey(0)\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ocr_venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
